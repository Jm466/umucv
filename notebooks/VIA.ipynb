{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/albertoruiz/umucv/master/images/demos/FIUM.png\" width=\"350px\" class=\"pull-right\" style=\"display: inline-block\">\n",
    "\n",
    "# Visión Artificial\n",
    "\n",
    "### 4º de Grado en Ingeniería Informática\n",
    "\n",
    "Curso 2023-2024<br>\n",
    "Prof. [*Alberto Ruiz*](http://dis.um.es/profesores/alberto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concept map](https://raw.githubusercontent.com/albertoruiz/umucv/master/images/demos/concept_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Szeliski: Computer Vision: Algorithms and Applications, 2nd ed.](https://szeliski.org/Book/)\n",
    "- [OpenCV](https://opencv.org/): [tutoriales en Python](https://docs.opencv.org/4.7.0/d6/d00/tutorial_py_root.html), [documentación](https://docs.opencv.org/4.7.0/), [libro1](https://books.google.es/books?id=seAgiOfu2EIC&printsec=frontcover), [libro2](https://books.google.es/books?id=9uVOCwAAQBAJ&printsec=frontcover), [libro3](https://books.google.es/books?id=iNlOCwAAQBAJ&printsec=frontcover)\n",
    "- [Bishop: Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "- [Bishop: Deep Learning](https://www.bishopbook.com/)\n",
    "- [Python](https://docs.python.org/3.10/), [numpy](http://www.numpy.org/), [matplotlib](http://matplotlib.org/index.html)\n",
    "- [scikit-image](http://scikit-image.org/), [scikit-learn](http://scikit-learn.org), [scipy](http://docs.scipy.org/doc/scipy/reference/)\n",
    "- [datasets](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Image_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El símbolo \"→\" indica enlace a los notebooks utilizados en cada capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Presentación (22/1/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ introducción](intro.ipynb), [→ instalación](install.ipynb), [→ Python](python.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introducción a la asignatura\n",
    "- Repaso de Python, numpy y matplotib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introducción a la imagen digital (29/1/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ imagen](imagen.ipynb), [→ gráficas](graphs.ipynb), [→ indexado/stacks](stacks.ipynb), [→ dispositivos de captura](captura.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelo pinhole. Campo de visión (FOV, *field of view*, parámetro $f$)\n",
    "- Imagen digital: rows, cols, depth, step. Planar or pixel order. Tipo de pixel: byte vs float\n",
    "- Color encoding: RGB vs YUV vs HSV\n",
    "- Coordendas de pixel, coordenadas normalizadas (indep de resolución), coordenadas calibradas (independiente del FOV).\n",
    "- Aspect ratio. Resize.\n",
    "- Manipulación: slice regions, \"stack\" de imágenes\n",
    "- primitivas gráficas\n",
    "- captura: webcams, cameras ip, archivos de vídeo, v4l2-ctl, etc. Load / save.\n",
    "- entornos de conda, pyqtgraph, pycharm, spyder\n",
    "- Herramientas: formatos de imagen, imagemagick, gimp, mplayer/mencoder/ffmpeg, mpv, gstreamer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Segmentación por color (5/2/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ canales de color](color.ipynb), [→ histograma](histogram.ipynb), [→ efecto chroma](chroma.ipynb), [→ segmentación por color](colorseg.ipynb), [→ cuantización de color](codebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Teoría del color\n",
    "- ROIs, masks, probability map, label map\n",
    "- Componentes conexas vs contornos.\n",
    "- inRange\n",
    "- Chroma key\n",
    "- Histograma, transformaciones de valor (brillo, contraste), ecualización\n",
    "- Histograma nD\n",
    "- Distancia entre histogramas. Reproyección de histograma\n",
    "- background subtraction\n",
    "- activity detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtros digitales (12/2/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ filtros de imagen](filtros.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lineal\n",
    "\n",
    "    - convolution\n",
    "    - máscaras para paso alto, bajo, etc.\n",
    "    - separabilidad\n",
    "    - integral image, box filter\n",
    "    - dominio frecuencial\n",
    "    - filtrado inverso\n",
    "\n",
    "\n",
    "- no lineal\n",
    "\n",
    "    - mediana\n",
    "    - min, max\n",
    "    - algoritmos generales\n",
    "\n",
    "\n",
    "- Gaussian filter\n",
    "\n",
    "    - separabilidad\n",
    "    - cascading\n",
    "    - Fourier\n",
    "    - scale space\n",
    "\n",
    "\n",
    "\n",
    "- [morphological operations](http://docs.opencv.org/master/d9/d61/tutorial_py_morphological_ops.html#gsc.tab=0)\n",
    "\n",
    "    - structuring element\n",
    "    - dilate, erode\n",
    "    - open, close\n",
    "    - gradient\n",
    "    - fill holes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis frecuencial (19/2/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[→ análisis frecuencial](fourier.ipynb), [→ filtrado inverso](inversefilt.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reconocimiento de formas (26/2/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ shapes](shapes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- umbralización\n",
    "- análisis de regiones (componentes conexas, transformada de distancia)\n",
    "- manipulación de contornos\n",
    "- invariantes frecuenciales de forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Detección de bordes (4/3/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ detección de bordes](bordes.ipynb), [→ Canny nms en C](cannyC.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradiente: visualización como *vector field*\n",
    "- operador de Canny\n",
    "- transformada de Hough\n",
    "- Histograma de orientaciones del gradiente (HOG)\n",
    "- implementación simple de HOG\n",
    "- detección de *pedestrians*\n",
    "- face landmarks (dlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Flujo óptico (11/3/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[→ elipse de incertidumbre](covarianza.ipynb), [→ optical flow](harris.ipynb)\n",
    "\n",
    "- elipse de incertidumbre\n",
    "- cross-correlation\n",
    "- corners (Harris)\n",
    "- Lucas-Kanade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. *Keypoints* (18/3/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ keypoints](keypoints.ipynb), [→ bag of visual words](bag-of-words.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modelo cuadrático\n",
    "- blobs / saddle points (Hessian)\n",
    "- SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. *Machine learning* (8/4/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ machine learning](machine-learning.ipynb) \n",
    "\n",
    "- Repaso de *Machine Learning* y *Pattern Recognition*\n",
    "- Repaso de computación neuronal\n",
    "- Introducción a la redes convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. *Deep learning* en visión artificial (15/4/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ modelos avanzados](deep.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11a. Coordenadas homogéneas (22/4/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos el estudio de la geometría visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ perspectiva](geovis.ipynb), [→ coordenadas homogéneas](coordhomog.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones lineales\n",
    "\n",
    "- espacios lineales, vectores\n",
    "- transformaciones lineales, matrices\n",
    "- producto escalar (**dot** product)\n",
    "- producto vectorial (**cross** product)\n",
    "- puntos, rectas, planos, meet & join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometría del plano\n",
    "\n",
    "- coordenadas homogéneas\n",
    "- interpretación como rayos\n",
    "- puntos y rectas del plano\n",
    "- incidencia e intersección, dualidad\n",
    "- puntos del infinito, recta del infinito\n",
    "- manejo natural de puntos del infinito\n",
    "- horizonte de un plano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11b. Transformaciones del plano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ transformaciones de dominio](lookup.ipynb), [→ transformaciones del plano](transf2D.ipynb), [→ sistemas de ecuaciones](sistecs.ipynb), [→ DLT](DLT.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Desplazamientos, rotaciones, escalado uniforme, escalado general, proyectividad.\n",
    "- Grupos euclídeo, similar, afín, proyectivo.\n",
    "- Propiedades invariantes de cada grupo.\n",
    "- Representación como matriz homogénea $3\\times 3$ y tipos de matriz de cada grupo.\n",
    "- *Cross ratio* de 4 puntos en una recta. De 5 rectas.\n",
    "- Estimación de transformaciones a partir de correspondencias.\n",
    "- Aplicaciones: rectificación de planos, mosaico de imágenes.\n",
    "- Transformaciones de dominio (deformaciones), lookup table.\n",
    "\n",
    "Avanzado\n",
    "\n",
    "- Transformación de rectas. Covarianza y contravarianza.\n",
    "- Cónicas: incidencia, tangencia, (pole-polar), cónica dual, transformación.\n",
    "- Objetos invariantes en cada grupo de transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12a. Modelo de cámara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ modelo de la cámara](camera.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Espacio proyectivo: puntos y líneas 3D, planos, grados de libertad, plano del infinito, analogía con 2D.\n",
    "- Grupos de transformaciones 3D: y sus invariantes.\n",
    "- Modelo pinhole (proyección), cámara oscura, lente.\n",
    "- Transformación de perspectiva: proyección $\\mathcal P^3 \\rightarrow\\mathcal P ^2$.\n",
    "- cámara calibrada C=PRT, 6 dof, parámetros extrínsecos o pose.\n",
    "- calibración, distorsión radial.\n",
    "- Matriz de cámara estándar $M=K[R|t]$.\n",
    "- Matriz de calibración $K$ y campo visual.\n",
    "- PnP (*pose from n points*).\n",
    "- Realidad aumentada.\n",
    "- Anatomía de la cámara\n",
    "- Rotaciones sintéticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12b. Visión estéreo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ stereo](stereo.ipynb), [→ stereo-challenge](stereo-challenge.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Triangulación\n",
    "- Geometría epipolar\n",
    "- Extracción de cámaras\n",
    "- Rectificación estéreo\n",
    "- Mapas de profundidad\n",
    "\n",
    "\n",
    "Experimentos\n",
    "\n",
    "- Reproduce los experimentos con un par estéreo tomado con tu propia cámara usando el *tracker* de puntos estudiado en una clase anterior.\n",
    "- Intenta poner en marcha el sistema [VisualSFM](http://ccwu.me/vsfm/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Otras técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ textura](textura.ipynb), [→ varios](varios.ipynb), [→ inpainting](ipmisc.ipynb), [→ transformada de distancia](transf-dist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clasificación de texturas mediante *LBP* (Wang and He, 1990, [wiki](https://en.wikipedia.org/wiki/Local_binary_patterns))\n",
    "- Detección de caras mediante *adaboost* ([Viola & Jones, 2001](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.6807), [wiki](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework))\n",
    "- Herramientas para OCR (*[tesseract](https://github.com/tesseract-ocr)*)\n",
    "- Herramientas para códigos de barras y QR (*[zbar](http://zbar.sourceforge.net/)*)\n",
    "- Segmentación de objetos mediante *GrabCut* ([Rother et al. 2004](https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf), [tutorial](http://docs.opencv.org/3.2.0/d8/d83/tutorial_py_grabcut.html))\n",
    "- Transformada de distancia\n",
    "- Detección de elipses\n",
    "- Inpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [Guion de las sesiones](guionpracticas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobación inicial\n",
    "\n",
    "1. [`hello.py`](../code/hello.py): lee imagen de archivo, la reescala, muestra y sobreescribe un texto.\n",
    "\n",
    "1. [`webcam.py`](../code/webcam.py): muestra la secuencia de imágenes capturadas por una webcam.\n",
    "\n",
    "1. [`2cams.py`](../code/2cams.py): combina las imágenes tomadas por dos cámaras.\n",
    "\n",
    "1. [`stream.py`](../code/stream.py): ejemplo de uso de la fuente genérica de imágenes.\n",
    "\n",
    "1. [`surface.py`](../code/surface.py): superficie 3D de niveles de gris en vivo usando pyqtgraph.\n",
    "\n",
    "1. [`facemesh.py`](../code/facemesh.py): malla de una cara usando mediapipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilidades\n",
    "\n",
    "1. [`save_video.py`](../code/save_video.py): ejemplo de uso de la utilidad de grabación de vídeo.\n",
    "\n",
    "1. [`mouse.py`](../code/mouse.py), [`medidor.py`](../code/medidor.py): ejemplo de captura de eventos de ratón.\n",
    "\n",
    "1. [`roi.py`](../code/roi.py): ejemplo de selección de región rectangular.\n",
    "\n",
    "1. [`trackbar.py`](../code/trackbar.py): ejemplo de parámetro interactivo.\n",
    "\n",
    "1. [`help_window.py`](../code/help_window.py): ejemplo de ventana de ayuda.\n",
    "\n",
    "1. [`wzoom.py`](../code/wzoom.py): ejemplo de ventana con zoom.\n",
    "\n",
    "1. [`args.py`](../code/args.py):  ejemplo de argumentos en la línea de órdenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actividad\n",
    "\n",
    "1. [`deque.py`](../code/deque.py): procesamiento de las $n$ imágenes más recientes.\n",
    "\n",
    "1. [`backsub0.py`](../code/backsub0.py), [`backsub.py`](../code/backsub.py): eliminación de fondo mediante MOG2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color\n",
    "\n",
    "1. [`histogram.py`](../code/histogram.py): histograma en vivo con opencv.\n",
    "\n",
    "1. [`histogram2.py`](../code/histogram2.py): histograma en vivo con matplotlib.\n",
    "\n",
    "1. [`inrange0.py`](../code/inrange0.py), [`inrange.py`](../code/inrange.py): umbralización de color, máscaras, componentes conexas y contornos.\n",
    "\n",
    "1. [`reprohist.py`](../code/reprohist.py),  [`mean-shift.py`](../code/mean-shift.py), [`camshift.py`](../code/camshift.py): reproyección de histograma y tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. boxy.py: combinación simple de filtros para detección de zonas \"interesantes\".\n",
    "\n",
    "1. box_vs_gaussian.py: comparación de calidad y eficiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`hog/pedestrian.py`](../code/hog/pedestrian.py): detector de peatones de opencv.\n",
    "\n",
    "1. [`hog/facelandmarks.py`](../code/hog/facelandmarks.py): detector de caras y landmarks de dlib.\n",
    "\n",
    "1. [`hog/hog0.py`](../code/hog/hog0.py): experimentos con hog.\n",
    "\n",
    "1. [`regressor.py`](../code/regressor.py): predictor directo de la posición de una región."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`crosscorr.py`](../code/crosscorr.py): ejemplo de match template.\n",
    "\n",
    "1. [`kcf.py`](../code/crosscorr.py): ejemplo de discriminative correlation filter.\n",
    "\n",
    "1. [`LK/*.py`](../code/LK): seguimiento de puntos con el método de Lucas-Kanade.\n",
    "\n",
    "1. [`SIFT/*.py`](../code/sift.py): demostración de la detección de keypoints y búsqueda de coincidencias en imágenes en vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`shape/*.py`](../code/shape): reconocimiento de formas mediante descriptores frecuenciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`code/DL`](../code/DL): Modelos avanzados de deep learning para visión artificial (mediapipe, inception, YOLO, FaceDeep, openpose)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`code/polygons`](../code/polygons) y [`code/elipses`](../code/elipses): Rectificación de planos en base a marcadores artificiales.\n",
    "\n",
    "1. [`stitcher.py`](../code/stitcher.py): construcción automática de panoramas.\n",
    "\n",
    "1. [`code/pose`](../code/pose): estimación de la matriz de cámara y realidad aumentada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`surface2.py`](../code/surface2.py): superficie 3D de niveles de gris  suavizada y manejo de teclado con pyqtgraph y opengl.\n",
    "\n",
    "1. [`server.py`](../code/server.py): ejemplo de servidor web de imágenes capturadas con la webcam.\n",
    "\n",
    "1. [`mjpegserver.py`](../code/mjpegserver.py): servidor de secuencias de video en formato mjpeg.\n",
    "\n",
    "1. [`bot`](../code/bot): bots de [Telegram](https://python-telegram-bot.org/).\n",
    "\n",
    "1. [`grabcut.py`](../code/grabcut.py): segmentación de objetos interactiva mediante GrabCut.\n",
    "\n",
    "1. [`spectral.py`](../code/spectral.py): FFT en vivo.\n",
    "\n",
    "1. [`thread`](../code/thread): captura y procesamiento concurrente.\n",
    "\n",
    "1. [`testC.py`](../code/testC.py), [`inC`](../code/inC): Interfaz C-numpy.\n",
    "\n",
    "1. [`ocr.py`](../code/ocr.py): reconocimiento de caracteres impresos con tesseract/tesserocr sobre imagen en vivo.\n",
    "\n",
    "1. [`zbardemo.py`](../code/zbardemo.py): detección de códigos de barras y QR sobre imagen en vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entrega de los ejercicios se hará en una tarea del aula virtual dentro de un archivo comprimido.\n",
    "\n",
    "Debe incluir el **código** completo .py de todos los ejercicios, los ficheros auxiliares (siempre que no sean muy pesados), y una **memoria** con una **explicación** detallada de las soluciones propuestas, las funciones o trozos de código más importantes, y **resultados** de funcionamiento con imágenes de evaluación **originales** en forma de pantallazos o videos de demostración. También es conveniente incluir información sobre tiempos de cómputo, limitaciones de las soluciones propuestas y casos de fallo.\n",
    "\n",
    "La memoria se presentará en un formato **pdf** o **jupyter** (en este caso se debe adjuntar también una versión **html** del notebook completamente evaluado).\n",
    "\n",
    "Lo importante, además de la evaluación de la asignatura, es que os quede un buen documento de referencia para el futuro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Preguntas frecuentes](FAQ.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obligatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS**: Amplia el ejemplo `code/DL/hands/mano.py` hecho en clase para reconocer gestos simples, como por ejemplo contar el número de dedos extendidos. Haz un controlador sin contacto de varios grados de libertad que mida, al menos, distancia de la mano a la cámara y ángulo de orientación. Utilízalo para controlar alguno de tus programas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTROS**. Muestra en vivo el efecto de diferentes filtros, seleccionando con el teclado el filtro deseado y modificando sus parámetros (p.ej. el nivel de suavizado) con trackbars. Aplica el filtro en un ROI para comparar el resultado con el resto de la imagen ([ejemplo](../images/demos/ej-c4.png))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASIFICADOR**. Prepara una aplicación sencilla de reconocimiento de imágenes. Debe admitir (al menos) dos argumentos:\n",
    "\n",
    "`--models=<directorio>`, la carpeta donde hemos guardado un conjunto de imágenes de objetos o escenas que queremos reconocer.\n",
    "\n",
    "`--method=<nombre>`, el nombre de un método de comparación. \n",
    "\n",
    "Cada fotograma de entrada se compara con los modelos utilizando el método seleccionado y se muestra información sobre el resultado (el modelo más parecido o probable, las distancias a los diferentes modelos, alguna medida de confianza, etc.). Implementa inicialmente un método basado en el \"embedding\" obtenido por [mediapipe](https://developers.google.com/mediapipe/solutions/vision/image_embedder) (`code/DL/embbeder`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIFT**. Añade al ejercicio CLASIFICADOR un método basado en el número de coincidencias de *keypoints* SIFT. Utilízalo para reconocer objetos con bastante textura (p. ej. carátulas de CD, portadas de libros, cuadros de pintores, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECTIF**. Rectifica la imagen de un plano para medir distancias (tomando manualmente referencias conocidas). Por ejemplo, mide la distancia entre las monedas en `coins.png` o la distancia a la que se realiza el disparo en `gol-eder.png`. Las coordenadas reales de los puntos de referencia y sus posiciones en la imagen deben pasarse como parámetro en un archivo de texto. Aunque puedes mostrar la imagen rectificada para comprobar las operaciones, debes marcar los puntos y mostrar el resultado sobre la imagen original. Verifica los resultados con **imágenes originales** tomadas por ti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RA**. Crea un efecto de realidad aumentada en el que el usuario desplace objetos virtuales hacia posiciones marcadas con el ratón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAPA**: Amplia el ejemplo `code/medidor.py` para convertir la distancia entre dos pixels marcados con el ratón en el ángulo que forman los rayos ópticos correspondientes, sabiendo el campo visual (FOV) de la cámara. Utiliza el script para encontrar mediante una construcción geométrica la posición aproximada en un mapa desde la que se ha tomado una imagen en la que se ven algunos puntos característicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTROS II**: **a)** Comprueba la propiedad de \"cascading\" del filtro gaussiano. **b)** Comprueba la propiedad de \"separabilidad\" del filtro gaussiano. **c)** Implementa en Python dede cero (usando bucles) el algoritmo de convolución con una máscara general y compara su eficiencia con la versión de OpenCV. **c)** Impleméntalo en C y haz un \"wrapper\" para utilizarlo desde Python (consulta al profesor). **d)** Implementa el box filter con la imagen integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POLYGON**: Implementa un procedimiento para mejorar la aproximación poligonal obtenida por el método `cv.approxPolyDP` permitiendo vértices fuera del contorno de entrada. Da una medida de la calidad de la aproximación. Pruébalo construyendo un cuadrilátero a partir de la silueta de un carnet o una tarjeta con las esquinas redondeadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VROT**. Amplía el *tracker* de Lucas-Kanade (ejemplo `LK/lk_track.py`) para **a)** determinar en qué dirección se mueve la cámara (UP, DOWN, LEFT, RIGHT, FORWARD, BACKWARD); **b)** estimar de forma aproximada la velocidad angular de rotación de la cámara (grados/segundo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DL**. Entrena un modelo de deep learning con tus propias imágenes. Puedes utilizar la herramienta ultralytics, siguiendo [este ejemplo](https://docs.ultralytics.com/modes/train/#usage-examples), o el modelo propuesto en `code/DL/UNET`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUDOKU**. Haz un programa capaz de rellenar con realidad aumentada un sudoku que se observa en una imagen en vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CARD**. Sustituye la foto de tu carnet de la UMU que se observa en una imagen en vivo. Intenta conseguir un resultado parecido a `../images/demos/dni.mp4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PANO**. Crea automáticamente un mosaico a partir de las imágenes en una carpeta. Las imágenes no tienen por qué estar ordenadas ni formar una cadena lineal y no sabemos el espacio que ocupa el resultado. El usuario debe intervenir lo menos posible. Recuerda que debe tratarse de una escena plana o de una escena cualquiera vista desde el mismo centro de proyección. Debes usar homografías. Compara el resultado con el que obtiene la utilidad de stitching de OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL3D**. Utiliza alguna de las herramientas [colmap](https://colmap.github.io/) o [meshroom](https://meshroom-manual.readthedocs.io/en/latest/) para construir un modelo 3D de un objeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIBRE**. Resuelve un problema de visión artificial de tu elección. Solo se valorarán las aportaciones originales."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
